---
title: "Matrix Algebra"
author: "Kelly Robbins"
date: "2025-01-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Matrices and Matrix Operations

## Why use matrices?

### - Compact representation 
### - Matrix algebra operations simplify computation
### - Lots of optimized algorithms for storing and operating with matrices

#### Here are the basics of matrix notation:

\begin{equation}
  \mathbf{A} = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9\\
  \end{bmatrix}
\end{equation}

\begin{equation}
  \mathbf{A_{i=2,j=3}} = 6
\end{equation}

\begin{equation}
  \mathbf{A} = \begin{bmatrix}
    \mathbf{B} & \mathbf{C} \\
  \end{bmatrix}
\end{equation}

\begin{equation}
 \mathbf{B } = \begin{bmatrix}
    1 & 2 \\
    4 & 5\\
    7 & 8
  \end{bmatrix}
\end{equation}

\begin{equation}
 \mathbf{C} = \begin{bmatrix}
    3 \\
    6\\
    9
  \end{bmatrix}
\end{equation}


## Matrix operations

### Transpose
$\mathbf{X}^{t} or \mathbf{X}^{'}$

\begin{equation}
  \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9 \\
  \end{bmatrix}^{t} =
  \begin{bmatrix}
    1 & 4 & 7 \\
    2 & 5 & 8 \\
    3 & 6 & 9 \\
  \end{bmatrix}
\end{equation}

### Multiplication
$\mathbf{Z}\mathbf{Y}$
\begin{equation}
  \begin{bmatrix}
    a & b  \\
    c & d \\
  \end{bmatrix}
  \begin{bmatrix}
    A \\
    B\\
  \end{bmatrix} =
  \begin{bmatrix}
    aA + bB \\
    cA + dB\\
  \end{bmatrix}
\end{equation}

### When multiplying two matrices the number of columns in the first matrix must equal the number of rows in the second matrix.

### Addition
$\mathbf{A} + \mathbf{B}$ 
\begin{equation}
  \begin{bmatrix}
    1 & 2  \\
    3 & 4 \\
  \end{bmatrix}
  +
  \begin{bmatrix}
    5 & 6\\
    7 & 8\\
  \end{bmatrix} =
  \begin{bmatrix}
    6 & 8 \\
    10 & 12\\
  \end{bmatrix}
\end{equation}

### When adding two matrices the number of coumns and rows must by the same in each matrix.

### Scalar by matrix
$b\mathbf{Q}$
\begin{equation}
  3
  \begin{bmatrix}
    3 & 9\\
    4 & 10\\
  \end{bmatrix} =
  \begin{bmatrix}
    9 & 27 \\
    12 & 30\\
  \end{bmatrix}
\end{equation}

---
# Special matrices

### Column
\begin{equation}
  \begin{bmatrix}
    3 \\
    4 \\
    5 \\
  \end{bmatrix}
\end{equation}


### Row 
\begin{equation}
  \begin{bmatrix}
    3  & 4 & 5\\
  \end{bmatrix}
\end{equation}

### Identity
\begin{equation}
  \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1 \\
  \end{bmatrix}
\end{equation}

### Symmetrical
\begin{equation}
  \begin{bmatrix}
    1 & 2 & 3 \\
    2 & 1 & 4 \\
    3 & 4 & 1 \\
  \end{bmatrix}
\end{equation}


## Inverse

### In linear algebra we use division to solve for unknown variables 

 $a*b = c$

 $\frac{a*b}{a}=\frac{c}{a}$

 $b=\frac{c}{a}$

### When using matrices we solve equations using the inverse.

The inverse of $\mathbf{A}$ is denoted as  $\mathbf{A^{-1}}$  

and 

$\mathbf{A}\mathbf{A^{-1}} = \mathbf{I}$

Lets look at an example

```{r inverse}
# first we allocate a matrix A
# There are several ways to do this but I will start by creat a 3x3 matrix of 0s
A=matrix(0,3,3)
# the first value is what I want stored in the matrix followed by the number of rows and columns
print(A)
# now set the values of A, one column at a time
# A[row,column]
A[,1] = c(3,2,1)
A[,2] = c(2,4,2)
A[,3] = c(1,2,4)
print("A")
print(A)
# here we calculate the inverse of A using the R function solve()
Ainv=solve(A)
print("A inverse")
print(Ainv)

# finally we multiply A by Ainv to show that we get the identity matrix
print("A multiple by its inverse")
print(A%*%Ainv)




```

Any matrix multiplied by the Identity matrix (with conforming dimensions) is equal to itself. Similar to dividing or multiplying a scalar by 1.

$\mathbf{A}\mathbf{I}=\mathbf{A}$

```{r identity}
# Calculating the Identity matrix by multiplying A by its inverse
I=A%*%Ainv

A%*%I
```





## Using matrices to represent a system of equations

### Let's look at an example of an experiment with 2 treatments and 8 observations. The first 4 observations were in treatment group 1 and the second 4 in treatment group 2.

#### Using scalar notation we would represent the equation as:

#### $y_{ij} = \beta_{i} + residual_{ij}$

#### We can write each of the 8 equations as:

#### $y_{11} = \beta_{1} + e_{11}$
#### $y_{12} = \beta_{1} + e_{12}$
#### $y_{13} = \beta_{1} + e_{13}$
#### $y_{14} = \beta_{1} + e_{14}$
#### $y_{21} = \beta_{2} + e_{21}$
#### $y_{22} = \beta_{2} + e_{22}$
#### $y_{23} = \beta_{2} + e_{23}$
#### $y_{24} = \beta_{2} + e_{24}$

#### By introducing and indicator variable taking on the value of 1 or 0 we can rewrite the above equations as:


#### $y_{11} = 1*\beta_{1} + 0*\beta_{2} + e_{11}$
#### $y_{12} = 1*\beta_{1} + 0*\beta_{2} + e_{12}$
#### $y_{13} = 1*\beta_{1} + 0*\beta_{2} + e_{13}$
#### $y_{14} = 1*\beta_{1} + 0*\beta_{2} + e_{14}$
#### $y_{21} = 0*\beta_{1} + 1*\beta_{2} + e_{21}$
#### $y_{22} = 0*\beta_{1} + 1*\beta_{2} + e_{22}$
#### $y_{23} = 0*\beta_{1} + 1*\beta_{2} + e_{23}$
#### $y_{24} = 0*\beta_{1} + 1*\beta_{2} + e_{24}$

#### Using the second form of the equations and the properties of matrix multiplication we represent the set of equations in matrix form as 

$\mathbf{y}=\mathbf{X}\mathbf{\beta}+\mathbf{e}$

#### Where:

$\mathbf{X} =$
 \begin{equation}
  \begin{bmatrix}
    1 & 0 \\
    1 & 0 \\
    1 & 0 \\
    1 & 0 \\
    0 & 1 \\
    0 & 1 \\
    0 & 1 \\
    0 & 1 \\
  \end{bmatrix}
 \end{equation}

$\mathbf{\beta} =$
\begin{equation}
 \begin{bmatrix}
  \beta_{1} \\
  \beta_{2} \\
  \end{bmatrix}
\end{equation}
 
$\mathbf{y} =$
\begin{equation}
  \begin{bmatrix}
  y_{1} \\
  y_{2} \\
  y_{3} \\
  y_{4} \\
  y_{5} \\
  y_{6} \\
  y_{7} \\
  y_{8} \\
  \end{bmatrix}
\end{equation}

$\mathbf{e} =$
\begin{equation}
  \begin{bmatrix}
  e_{1} \\
  e_{2} \\
  e_{3} \\
  e_{4} \\
  e_{5} \\
  e_{6} \\
  e_{7} \\
  e_{8} \\
  \end{bmatrix}
\end{equation}


#### Here $\mathbf{X}$ is called a design matrix. It's purpose is to map values in $\mathbf{y}$ to the variables in $\mathbf{\beta}$ 

### Solving for the two treatment effects.

#### In scalar notation the solutions for the two treatment effects are:

#### $\hat{\beta_{1}} = \frac{\sum_{j=1}^{n_{j}} y_{1j}}{n_{j}}$
#### and
#### $\hat{\beta_{2}} = \frac{\sum_{j=1}^{n_{j}} y_{2j}}{n_{j}}$





## Homework

### For the homework assignment we will simulate data, contruct the corresponding design matrix, and use matrix algebra to estimate the treatment effects. The simulated dataset will have 18 obersevations from 3 treatments.

1) Allocate a vector $\mathbf{y}$ to store the 18 phenotypes and then simulate observations from each treatment group. (2 pts)
    a. Sample y1-y6 (given treatment 1) from a normal distribution with mean 3 and variance 1
    b. Sample y7-y12 (given treatment 2) from a normal distribution with mean 9 and variance 1
    c. Sample y13-y18 (given treatment 3) from a normal distribution with mean 14 and variance 1

```{r}
y <- as.matrix(rep(0,18))
y[1:6] = rnorm(n = 6, mean = 3, sd = sqrt(1))
y[7:12] = rnorm(n = 6, mean = 9, sd = sqrt(1))
y[13:18] = rnorm(n = 6, mean = 14, sd = sqrt(1))
print(y)
```



2) Set up the appropriate design matrix $\mathbf{X}$ for the simulated data $\mathbf{y}$. (2 pts) 

```{r}
X <- matrix(0, nrow = 18, ncol = 3)
#assign first 6 values of Y into column 1 of X and rest zero's in column 1
X[1:6, 1] <- 1
#assign values from 7 to 12 of Y into column 2 of X at 7 to 12th position
X[7:12, 2] <- 1
#similarly, 13 to 18th values of y into 3rd column of X as 13th to 18th elements
X[13:18, 3] <- 1
print(X)
```

3) Calculate $\mathbf{X^t}\mathbf{y}$. What does each value in $\mathbf{X^t}\mathbf{y}$ represent? (2 pts)

Represent: sum of the response variable (y) for each treatment 1, 2 and 3 respectively as defined in columns of X.i.e. the first value of 15.75 is the sum of y for Treatment 1 and so on for 2nd and 3rd treatments.

```{r}
# X is 18*3 so Xt will be 3*18 and when multiplied with y (18*1) will result in 3*1 matrix
Xt_y <- t(X) %*% y
Xt_y
```


4) Calculate $\mathbf{X^t}\mathbf{X}$. What does each value in $\mathbf{X^t}\mathbf{X}$ represent? (2 pts)

Answer: 

- XtX represents the structure of the design matrix X where the diagonal elements (in the main diagonal) are the number of observations in each treatment or group.

- Non diagonal elements being 0 suggest there are no other factor in the equation or there are factors but no common observations with X treatment.
For eg. if i had fertilizer treatment and also location as a factor, then other non-diagonal element values will suggest how many y values have both location and fertilizer treatment. 

```{r }
Xt_X <- t(X) %*% X
Xt_X
```

5) Using $\mathbf{X^t}\mathbf{X}$ and $\mathbf{X^t}\mathbf{y}$ calculate $\mathbf{\hat{\beta}}$ where $\hat{\beta_{1}} = \frac{\sum_{j=1}^{6} y_{1j}}{6}$, $\hat{\beta_{2}} = \frac{\sum_{j=1}^{6} y_{2j}}{6}$, and $\hat{\beta_{3}} = \frac{\sum_{j=1}^{6} y_{3j}}{6}$. (2 pts)

The formula is : Î²hat =(X'X)' X'y

These coefficients (beta_hat1,beta_hat2,beta_hat3) are the estimated coefficients of the linear model which represent the mean phenotype for each treatment group. Eg. 2.62 is the mean phenotype estimated for treatment 1. 


### solve for beta using least squares estimates of beta

```{r solve for beta using least squares estimates of beta}
# the formula is 
b_hat <- solve(Xt_X) %*% (Xt_y)
b_hat
```

### Another way of calculating beta but more steps and complicated

```{r another way of calculating beta but more steps and complicated}
beta_1_hat = Xt_y[1,1]/Xt_X[1,1]
#beta_1_hat
beta_2_hat = Xt_y[2,1]/6
#beta_2_hat
beta_3_hat = Xt_y[3,1]/6
#beta_3_hat
beta <- matrix(0, nrow = 3, ncol = 1)
beta[1,1] <- beta_1_hat
beta[2,1] <- beta_2_hat
beta[3,1] <- beta_3_hat
beta
```

